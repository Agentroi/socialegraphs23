{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'Response' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mget(episode_url)\n\u001b[1;32m     17\u001b[0m \u001b[39m# Parse the content using BeautifulSoup\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m soup \u001b[39m=\u001b[39m BeautifulSoup(response, \u001b[39m'\u001b[39;49m\u001b[39mhtml.parser\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     20\u001b[0m \u001b[39mprint\u001b[39m(soup)\n\u001b[1;32m     22\u001b[0m \u001b[39m# Function to extract all episode links from the page\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/bs4/__init__.py:315\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(markup, \u001b[39m'\u001b[39m\u001b[39mread\u001b[39m\u001b[39m'\u001b[39m):        \u001b[39m# It's a file-type object.\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     markup \u001b[39m=\u001b[39m markup\u001b[39m.\u001b[39mread()\n\u001b[0;32m--> 315\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39;49m(markup) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m256\u001b[39m \u001b[39mand\u001b[39;00m (\n\u001b[1;32m    316\u001b[0m         (\u001b[39misinstance\u001b[39m(markup, \u001b[39mbytes\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39m<\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m markup)\n\u001b[1;32m    317\u001b[0m         \u001b[39mor\u001b[39;00m (\u001b[39misinstance\u001b[39m(markup, \u001b[39mstr\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m<\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m markup)\n\u001b[1;32m    318\u001b[0m ):\n\u001b[1;32m    319\u001b[0m     \u001b[39m# Issue warnings for a couple beginner problems\u001b[39;00m\n\u001b[1;32m    320\u001b[0m     \u001b[39m# involving passing non-markup to Beautiful Soup.\u001b[39;00m\n\u001b[1;32m    321\u001b[0m     \u001b[39m# Beautiful Soup will still parse the input as markup,\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[39m# since that is sometimes the intended behavior.\u001b[39;00m\n\u001b[1;32m    323\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_markup_is_url(markup):\n\u001b[1;32m    324\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_markup_resembles_filename(markup)                \n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'Response' has no len()"
     ]
    }
   ],
   "source": [
    "# To start with, I'll parse the HTML file to understand its structure and content.\n",
    "# I will use BeautifulSoup to parse the HTML and identify the scene shifts and characters.\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Let's start by parsing the uploaded HTML file to find all the links corresponding to the episodes.\n",
    "# Each episode link has a text format like \"1x01\", \"1x02\", etc.\n",
    "\n",
    "# The link to the specific episode transcript page\n",
    "episode_url = \"https://transcripts.foreverdreaming.org/viewtopic.php?t=7739\"\n",
    "\n",
    "# Make a request to get the content of the episode page\n",
    "response = requests.get(episode_url)\n",
    "\n",
    "# Parse the content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, 'html.parser')  # Use response.text for the HTML content\n",
    "\n",
    "\n",
    "print(soup)\n",
    "\n",
    "# Function to extract all episode links from the page\n",
    "def extract_episode_links(soup):\n",
    "    episode_links = {}\n",
    "\n",
    "    # Find all 'a' tags with href attribute\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        link_text = link.text.strip()\n",
    "        # Check if the link text matches the episode format (e.g., \"1x01\")\n",
    "        if link_text and 'x' in link_text and len(link_text.split('x')[0]) <= 2 and link_text.split('x')[1].isdigit():\n",
    "            season_number = int(link_text.split('x')[0])\n",
    "            # Correct the URL by combining the base URL with the href attribute\n",
    "            # Ensure that only the part after '/viewtopic.php' is appended to the base URL\n",
    "            url = 'https://transcripts.foreverdreaming.org' + link['href'][link['href'].find('/viewtopic.php'):]\n",
    "            # Organize links by season\n",
    "            if season_number not in episode_links:\n",
    "                episode_links[season_number] = []\n",
    "            episode_links[season_number].append(url)\n",
    "\n",
    "    return episode_links\n",
    "\n",
    "# Extracting the episode links\n",
    "episode_links = extract_episode_links(soup)\n",
    "\n",
    "# Display the first few links to verify\n",
    "{season: links[:2] for season, links in episode_links.items()}  # show first two links of each season for brevity\n",
    "# Make sure to replace 'your_directory_path' with the actual path where you want the folders to be created.\n",
    "base_directory_path = 'files/Game_of_Thrones_Transcripts'\n",
    "\n",
    "# This function will take the base directory and the links dictionary to download and organize the transcripts.\n",
    "def download_transcripts(base_directory, episode_links):\n",
    "    for season, links in episode_links.items():\n",
    "        # Create a directory for the season\n",
    "        season_directory = os.path.join(base_directory, f\"Season_{season}\")\n",
    "        os.makedirs(season_directory, exist_ok=True)\n",
    "\n",
    "        # Counter for episodes\n",
    "        episode_count = 1\n",
    "\n",
    "        for link in links:\n",
    "            # Make a request to the episode link\n",
    "            response = requests.get(link)\n",
    "            # Generate the filename using season and episode count\n",
    "            filename = f\"season_{season}_episode_{episode_count}.html\"\n",
    "            file_path = os.path.join(season_directory, filename)\n",
    "\n",
    "            # Write the content to the file\n",
    "            with open(file_path, 'w', encoding='utf-8') as file:\n",
    "                file.write(response.text)\n",
    "            print(f\"Downloaded {filename} in {season_directory}\")\n",
    "\n",
    "            # Increment the episode count\n",
    "            episode_count += 1\n",
    "            \n",
    "# Extracting the episode links from the HTML file (as done previously)\n",
    "\n",
    "# Calling the download function\n",
    "download_transcripts(base_directory_path, episode_links)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
